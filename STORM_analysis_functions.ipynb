{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec\n",
    "import scipy\n",
    "from scipy.signal import argrelextrema\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import maximum_filter\n",
    "from scipy.ndimage.morphology import generate_binary_structure, binary_erosion\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy import ndimage as ndi\n",
    "from __future__ import division\n",
    "from astropy.stats import RipleysKEstimator\n",
    "from sklearn.cluster import DBSCAN\n",
    "from __future__ import division\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(shape,size):\n",
    "    while shape/size!=shape//size:\n",
    "        shape=shape+1\n",
    "    return(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(array, nrows, ncols):\n",
    "    \"\"\"Split a matrix into sub-matrices.\"\"\"\n",
    "\n",
    "    r, h = array.shape\n",
    "    return (array.reshape(h//nrows, nrows, -1, ncols)\n",
    "                 .swapaxes(1, 2)\n",
    "                 .reshape(-1, nrows, ncols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_images(paths_data,paths_mask,size=50):\n",
    "    out=[]\n",
    "    for el1,el2 in zip(paths_data,paths_mask):\n",
    "        mtx=np.load(el1)\n",
    "        mask=np.load(el2)\n",
    "        zeros=np.zeros((resize(mtx.shape[0],size=size),resize(mtx.shape[0],size=size)))\n",
    "        zeros[0:mtx.shape[0],0:mtx.shape[0]]=mtx[0:mtx.shape[0],0:mtx.shape[0]]\n",
    "        \n",
    "        mask_zeros=np.full((resize(mtx.shape[0],size=size),resize(mtx.shape[0],size=size)), False, dtype=bool)\n",
    "        mask_zeros[0:mtx.shape[0],0:mtx.shape[0]]=mask[0:mask.shape[0],0:mask.shape[0]]\n",
    "        \n",
    "        a=split(zeros,size,size)\n",
    "        b=split(mask_zeros,size,size)\n",
    "        for slice1,slice2 in zip(a,b):\n",
    "            if len(np.where(slice2==True)[0])==size*size:\n",
    "                out.append(slice1)\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_function(crops_array,treshold,area,radii):\n",
    "    out=[]\n",
    "    out_df=pd.DataFrame()\n",
    "    Kest = RipleysKEstimator(area=area)\n",
    "    for im in crops_array:\n",
    "        z = np.random.uniform(low=0, high=area, size=(len(np.where(im>treshold)[0]), 2))\n",
    "        A=np.where(im>treshold)\n",
    "        data=np.concatenate((np.resize(A[0], len(A[0])).reshape(len(A[0]),1),np.resize(A[1], len(A[1])).reshape(len(A[1]),1)),axis=1)\n",
    "        L=Kest.Lfunction(data,radii)\n",
    "        L_rand=Kest.Lfunction(z,radii)\n",
    "        Norm=L/L_rand\n",
    "        out.append(Norm)\n",
    "        tmp_df=pd.DataFrame(Norm)\n",
    "        tmp_df[1]=np.array([x for x in range(len(radii))])\n",
    "        out_df=pd.concat([out_df,tmp_df])\n",
    "    return(np.nanmean(out,axis=0),out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairCorrelationFunction_2D(x, y, S, rMax, dr):\n",
    "    \"\"\"Compute the two-dimensional pair correlation function for a set of\n",
    "    spherical particles contained in a sqare with side length S.  This simple\n",
    "    function finds reference particles such that a sphere of radius rMax drawn\n",
    "    around the particle will fit entirely within the cube, eliminating the need\n",
    "    to compensate for edge effects.  If no such particles exist, an error is\n",
    "    returned.  Try a smaller rMax...or write some code to handle edge effects! ;)\n",
    "    Arguments:\n",
    "        x               an array of x positions of centers of particles\n",
    "        y               an array of y positions of centers of particles\n",
    "        S               length of each side of the cube in space\n",
    "        rMax            outer diameter of largest spherical shell\n",
    "        dr              increment for increasing radius of spherical shell\n",
    "    Returns a tuple: (g, radii, interior_indices)\n",
    "        g(r)            a numpy array containing the correlation function g(r)\n",
    "        radii           a numpy array containing the radii of the\n",
    "                        spherical shells used to compute g(r)\n",
    "        reference_indices   indices of reference particles\n",
    "    \"\"\"\n",
    "    from numpy import zeros, sqrt, where, pi, mean, arange, histogram\n",
    "\n",
    "    # Find particles which are close enough to the cube center that a sphere of radius\n",
    "    # rMax will not cross any face of the cube\n",
    "    bools1 = x > rMax\n",
    "    bools2 = x < (S - rMax) \n",
    "    bools3 = y > rMax\n",
    "    bools4 = y < (S - rMax)\n",
    "\n",
    "    interior_indices, = where(bools1 * bools2 * bools3 * bools4)\n",
    "    num_interior_particles = len(interior_indices)\n",
    "\n",
    "    if num_interior_particles < 1:\n",
    "        raise  RuntimeError (\"No particles found for which a sphere of radius rMax\\\n",
    "                will lie entirely within a cube of side length S.  Decrease rMax\\\n",
    "                or increase the size of the cube.\")\n",
    "\n",
    "    edges = arange(0., rMax + 1.1 * dr, dr)\n",
    "    num_increments = len(edges) - 1\n",
    "    g = zeros([num_interior_particles, num_increments])\n",
    "    radii = zeros(num_increments)\n",
    "    numberDensity = len(x) / S**2\n",
    "\n",
    "    # Compute pairwise correlation for each interior particle\n",
    "    for p in range(num_interior_particles):\n",
    "        index = interior_indices[p]\n",
    "        d = sqrt((x[index] - x)**2 + (y[index] - y)**2 )\n",
    "        d[index] = 2 * rMax\n",
    "\n",
    "        (result, bins) = histogram(d, bins=edges, normed=False)\n",
    "        g[p,:] = result / numberDensity\n",
    "\n",
    "    # Average g(r) for all interior particles and compute radii\n",
    "    g_average = zeros(num_increments)\n",
    "    for i in range(num_increments):\n",
    "        radii[i] = (edges[i] + edges[i+1]) / 2.\n",
    "        rOuter = edges[i + 1]\n",
    "        rInner = edges[i]\n",
    "        g_average[i] = mean(g[:, i]) / (4.0 / 3.0 * pi * (rOuter**3 - rInner**3))\n",
    "\n",
    "    return (g_average, radii, interior_indices)\n",
    "    # Number of particles in shell/total number of particles/volume of shell/number density\n",
    "    # shell volume = 4/3*pi(r_outer**3-r_inner**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RDF_function(crops_array,treshold,area,rMax,dr):\n",
    "    out=[]\n",
    "    out_df=pd.DataFrame()\n",
    "    for im in crops_array:\n",
    "        z = np.random.uniform(low=0, high=area, size=(len(np.where(im>treshold)[0]), 2))\n",
    "        A=np.where(im>treshold)\n",
    "        data=np.concatenate((np.resize(A[0], len(A[0])).reshape(len(A[0]),1),np.resize(A[1], len(A[1])).reshape(len(A[1]),1)),axis=1)\n",
    "        RDF=pairCorrelationFunction_2D(data[:,0],data[:,1],S=area,rMax=rMax,dr=dr)\n",
    "        RDF_rand=pairCorrelationFunction_2D(z[:,0],z[:,1],S=area,rMax=rMax,dr=dr)\n",
    "        Norm=RDF[0]/RDF_rand[0]\n",
    "        out.append(Norm)\n",
    "        tmp_df=pd.DataFrame(Norm,)\n",
    "        tmp_df[1]=np.array([x for x in range(rMax+1)])\n",
    "        out_df=pd.concat([out_df,tmp_df])\n",
    "    return(np.nanmean(out,axis=0),out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nanodomain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_mean_distance(data, cx, cy, i_centroid):\n",
    "    \"\"\"data - table after clustering\n",
    "    cx and cy coordinates of i centroid\n",
    "    i_centroid - label of centroid\n",
    "    in sklearn.Kmeans cluster_centers_ ordered by it's label e.g (0,1,2...N)\"\"\"\n",
    "    # Calculate Euclidean distance for each data point assigned to centroid\n",
    "    distances=[np.sqrt((x-cx)**2+(y-cy)**2) for (x, y) in data[data[4]==i_centroid][[1,2]].values]\n",
    "    # return all values\n",
    "    return distances\n",
    "\n",
    "def k_mean_distance_SD(data, cx, cy, i_centroid):\n",
    "    \"\"\"data - table after clustering\n",
    "    cx and cy coordinates of i centroid\n",
    "    i_centroid - label of centroid\n",
    "    in sklearn.Kmeans cluster_centers_ ordered by it's label e.g (0,1,2...N)\"\"\"\n",
    "    # Calculate Euclidean distance for each data point assigned to centroid\n",
    "    distances=[np.sqrt((x-cx)**2+(y-cy)**2) for (x, y) in data[data[4]==i_centroid][[1,2]].values]\n",
    "    # return SD to the center\n",
    "    return np.std(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_means_nanodomains(mtx,treshold,size):\n",
    "    \"\"\"find nanodomains in nuclei\n",
    "    return table with x,y,Intensity and cluster label and cluster_centers, \n",
    "    the last sorted by cluster num (0,1,2..)\n",
    "    return table with lables and cluster centers\"\"\"\n",
    "    #drop all intensities that < treshold\n",
    "    locs=np.where(mtx>treshold)\n",
    "    zeros=np.zeros(mtx.shape)\n",
    "    for x,y in zip(locs[0],locs[1]):\n",
    "        zeros[x,y]=mtx[x,y]\n",
    "    mtx=zeros\n",
    "    \n",
    "    i=[]\n",
    "    for x,y in zip(locs[0],locs[1]):\n",
    "        i.append(mtx[x,y])\n",
    "    I=np.array(i)\n",
    "    \n",
    "    #generate table of particles coordinates\n",
    "    table=pd.DataFrame()\n",
    "    table[1]=locs[0]\n",
    "    table[2]=locs[1]\n",
    "    table[3]=I\n",
    "    # find local maximas\n",
    "    detected_peaks=peak_local_max(mtx,size)\n",
    "    \n",
    "    #make a data table\n",
    "    data=pd.DataFrame(detected_peaks,columns=['y','x'])\n",
    "    #merge adjacent\n",
    "    # Compute DBSCAN\n",
    "    db = DBSCAN(eps=1, min_samples=1).fit(data)\n",
    "    labels = db.labels_\n",
    "    data['labels']=labels\n",
    "    #get new peaks\n",
    "    new_peaks=pd.DataFrame()\n",
    "    for l in data['labels'].drop_duplicates():\n",
    "        tmp=data[data['labels']==l]\n",
    "        new_peaks=pd.concat([new_peaks,pd.DataFrame(np.array([tmp.mean(axis=0).values]))])\n",
    "    new_peaks.columns=data.columns\n",
    "    kmeans = KMeans(n_clusters=len(new_peaks),init=new_peaks[['x','y']], \\\n",
    "                    random_state=0,precompute_distances='auto',algorithm='elkan').fit(table[[1,2]])\n",
    "    table[4]=kmeans.labels_\n",
    "    table.sort_values(by=4,inplace=True)\n",
    "    cluster_centers=kmeans.cluster_centers_\n",
    "    \n",
    "    return(table,cluster_centers)\n",
    "\n",
    "def K_means_nanodomains_SD(mtx,treshold,size):\n",
    "    \"\"\"find nanodomains in nuclei\n",
    "    return SD of distances of cluster points to cluster center\"\"\"\n",
    "    #drop all intensities that < treshold\n",
    "    locs=np.where(mtx>treshold)\n",
    "    zeros=np.zeros(mtx.shape)\n",
    "    for x,y in zip(locs[0],locs[1]):\n",
    "        zeros[x,y]=mtx[x,y]\n",
    "    mtx=zeros\n",
    "    \n",
    "    i=[]\n",
    "    for x,y in zip(locs[0],locs[1]):\n",
    "        i.append(mtx[x,y])\n",
    "    I=np.array(i)\n",
    "    \n",
    "    #generate table of particles coordinates\n",
    "    table=pd.DataFrame()\n",
    "    table[1]=locs[0]\n",
    "    table[2]=locs[1]\n",
    "    table[3]=I\n",
    "    # find local maximas\n",
    "    detected_peaks=peak_local_max(mtx,size)\n",
    "    \n",
    "    #make a data table\n",
    "    data=pd.DataFrame(detected_peaks,columns=['y','x'])\n",
    "    #merge adjacent\n",
    "    # Compute DBSCAN\n",
    "    db = DBSCAN(eps=1, min_samples=1).fit(data)\n",
    "    labels = db.labels_\n",
    "    data['labels']=labels\n",
    "    #get new peaks\n",
    "    new_peaks=pd.DataFrame()\n",
    "    for l in data['labels'].drop_duplicates():\n",
    "        tmp=data[data['labels']==l]\n",
    "        new_peaks=pd.concat([new_peaks,pd.DataFrame(np.array([tmp.mean(axis=0).values]))])\n",
    "    new_peaks.columns=data.columns\n",
    "    kmeans = KMeans(n_clusters=len(new_peaks),init=new_peaks[['x','y']], \\\n",
    "                    random_state=0,precompute_distances='auto',algorithm='elkan').fit(table[[1,2]])\n",
    "    table[4]=kmeans.labels_\n",
    "    table.sort_values(by=4,inplace=True)\n",
    "    cluster_centers=kmeans.cluster_centers_\n",
    "    \n",
    "    #SD for all clusters\n",
    "    SD=[]\n",
    "    for i, (cx, cy) in enumerate(cluster_centers):\n",
    "        SD.append(k_mean_distance_SD(table,cx,cy,i))\n",
    "    \n",
    "    return(np.array(SD))\n",
    "\n",
    "def k_mean_cluster_1NN(data, cx, cy):\n",
    "    \"\"\"data - table of cluster centers\n",
    "    cx and cy coordinates of i center\n",
    "    in sklearn.Kmeans cluster_centers_ ordered by it's label e.g (0,1,2...N)\"\"\"\n",
    "    # Calculate Euclidean distance for each data point assigned to centroid\n",
    "    distances=[np.sqrt((x-cx)**2+(y-cy)**2) for (x, y) in data]\n",
    "    # return all values\n",
    "    distances=np.sort(distances)\n",
    "    #first element is zero cluster with itself,second is minimum\n",
    "    return distances[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional functions with filtering option (by nanodomain size) for nanodomain analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_cluster_size(data,data_cluster_centers,min_size):\n",
    "    out=pd.DataFrame()\n",
    "    out_clusters=[]\n",
    "    j=0\n",
    "    for i, (cx, cy) in enumerate(data_cluster_centers):\n",
    "        tmp=data[data[4]==i]\n",
    "        if len(tmp)>=min_size:\n",
    "            j=j+1\n",
    "            tmp.loc[:,4]=np.array([j]*len(tmp))\n",
    "            tmp.loc[:,4]=tmp[4]-1\n",
    "            out=pd.concat([out,tmp])\n",
    "            out_clusters.append([cx,cy])\n",
    "    return(out,np.array(out_clusters))\n",
    "\n",
    "def k_mean_distance(data, cx, cy, i_centroid):\n",
    "    \"\"\"data - table after clustering\n",
    "    cx and cy coordinates of i centroid\n",
    "    i_centroid - label of centroid\n",
    "    in sklearn.Kmeans cluster_centers_ ordered by it's label e.g (0,1,2...N)\"\"\"\n",
    "    # Calculate Euclidean distance for each data point assigned to centroid\n",
    "    distances=[np.sqrt((x-cx)**2+(y-cy)**2) for (x, y) in data[data[4]==i_centroid][[1,2]].values]\n",
    "    # return all values\n",
    "    return distances\n",
    "\n",
    "def k_mean_distance_SD(data, cx, cy, i_centroid):\n",
    "    \"\"\"data - table after clustering\n",
    "    cx and cy coordinates of i centroid\n",
    "    i_centroid - label of centroid\n",
    "    in sklearn.Kmeans cluster_centers_ ordered by it's label e.g (0,1,2...N)\"\"\"\n",
    "    # Calculate Euclidean distance for each data point assigned to centroid\n",
    "    distances=[np.sqrt((x-cx)**2+(y-cy)**2) for (x, y) in data[data[4]==i_centroid][[1,2]].values]\n",
    "    # return SD to the center\n",
    "    return np.std(distances)\n",
    "\n",
    "def K_means_nanodomains_filter(mtx,treshold,size,min_size):\n",
    "    \"\"\"find nanodomains in nuclei\n",
    "    return table with x,y,Intensity and cluster label and cluster_centers, \n",
    "    the last sorted by cluster num (0,1,2..)\n",
    "    return table with lables and cluster centers\"\"\"\n",
    "    #drop all intensities that < treshold\n",
    "    locs=np.where(mtx>treshold)\n",
    "    zeros=np.zeros(mtx.shape)\n",
    "    for x,y in zip(locs[0],locs[1]):\n",
    "        zeros[x,y]=mtx[x,y]\n",
    "    mtx=zeros\n",
    "    \n",
    "    i=[]\n",
    "    for x,y in zip(locs[0],locs[1]):\n",
    "        i.append(mtx[x,y])\n",
    "    I=np.array(i)\n",
    "    \n",
    "    #generate table of particles coordinates\n",
    "    table=pd.DataFrame()\n",
    "    table[1]=locs[0]\n",
    "    table[2]=locs[1]\n",
    "    table[3]=I\n",
    "    # find local maximas\n",
    "    detected_peaks=peak_local_max(mtx,size)\n",
    "    \n",
    "    #make a data table\n",
    "    data=pd.DataFrame(detected_peaks,columns=['y','x'])\n",
    "    #merge adjacent\n",
    "    # Compute DBSCAN\n",
    "    db = DBSCAN(eps=1, min_samples=1).fit(data)\n",
    "    labels = db.labels_\n",
    "    data['labels']=labels\n",
    "    #get new peaks\n",
    "    new_peaks=pd.DataFrame()\n",
    "    for l in data['labels'].drop_duplicates():\n",
    "        tmp=data[data['labels']==l]\n",
    "        new_peaks=pd.concat([new_peaks,pd.DataFrame(np.array([tmp.mean(axis=0).values]))])\n",
    "    new_peaks.columns=data.columns\n",
    "    kmeans = MiniBatchKMeans(n_clusters=len(new_peaks),init=new_peaks[['x','y']], \\\n",
    "                    random_state=0).fit(table[[1,2]])\n",
    "    table[4]=kmeans.labels_\n",
    "    table.sort_values(by=4,inplace=True)\n",
    "    cluster_centers=kmeans.cluster_centers_\n",
    "    #filter by cluster size\n",
    "    return(filter_by_cluster_size(table,cluster_centers,min_size=min_size))\n",
    "\n",
    "def K_means_nanodomains_SD_filter(mtx,treshold,size,min_size):\n",
    "    \"\"\"find nanodomains in nuclei\n",
    "    return SD of distances of cluster points to cluster center\"\"\"\n",
    "    #drop all intensities that < treshold\n",
    "    locs=np.where(mtx>treshold)\n",
    "    zeros=np.zeros(mtx.shape)\n",
    "    for x,y in zip(locs[0],locs[1]):\n",
    "        zeros[x,y]=mtx[x,y]\n",
    "    mtx=zeros\n",
    "    \n",
    "    i=[]\n",
    "    for x,y in zip(locs[0],locs[1]):\n",
    "        i.append(mtx[x,y])\n",
    "    I=np.array(i)\n",
    "    \n",
    "    #generate table of particles coordinates\n",
    "    table=pd.DataFrame()\n",
    "    table[1]=locs[0]\n",
    "    table[2]=locs[1]\n",
    "    table[3]=I\n",
    "    # find local maximas\n",
    "    detected_peaks=peak_local_max(mtx,size)\n",
    "    \n",
    "    #make a data table\n",
    "    data=pd.DataFrame(detected_peaks,columns=['y','x'])\n",
    "    #merge adjacent\n",
    "    # Compute DBSCAN\n",
    "    db = DBSCAN(eps=1, min_samples=1).fit(data)\n",
    "    labels = db.labels_\n",
    "    data['labels']=labels\n",
    "    #get new peaks\n",
    "    new_peaks=pd.DataFrame()\n",
    "    for l in data['labels'].drop_duplicates():\n",
    "        tmp=data[data['labels']==l]\n",
    "        new_peaks=pd.concat([new_peaks,pd.DataFrame(np.array([tmp.mean(axis=0).values]))])\n",
    "    new_peaks.columns=data.columns\n",
    "    kmeans = MiniBatchKMeans(n_clusters=len(new_peaks),init=new_peaks[['x','y']], \\\n",
    "                    random_state=0).fit(table[[1,2]])\n",
    "    table[4]=kmeans.labels_\n",
    "    table.sort_values(by=4,inplace=True)\n",
    "    cluster_centers=kmeans.cluster_centers_\n",
    "    # filter by size\n",
    "    ntable,ncluster_centers=filter_by_cluster_size(table,cluster_centers,min_size=min_size)\n",
    "    #SD for all clusters\n",
    "    SD=[]\n",
    "    for i, (cx, cy) in enumerate(ncluster_centers):\n",
    "        SD.append(k_mean_distance_SD(ntable,cx,cy,i))\n",
    "    \n",
    "    return(np.array(SD))\n",
    "\n",
    "def k_mean_cluster_1NN(data, cx, cy):\n",
    "    \"\"\"data - table of cluster centers\n",
    "    cx and cy coordinates of i center\n",
    "    in sklearn.Kmeans cluster_centers_ ordered by it's label e.g (0,1,2...N)\"\"\"\n",
    "    # Calculate Euclidean distance for each data point assigned to centroid\n",
    "    distances=[np.sqrt((x-cx)**2+(y-cy)**2) for (x, y) in data]\n",
    "    # return all values\n",
    "    distances=np.sort(distances)\n",
    "    #first element is zero cluster with itself,second is minimum\n",
    "    return distances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
